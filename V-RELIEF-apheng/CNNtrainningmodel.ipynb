{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSXf1KGuDBfm",
        "outputId": "0717fe66-245e-4fd8-ac77-1f2278c25375",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.17.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (11.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.8.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->-r requirements.txt (line 1)) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->-r requirements.txt (line 1)) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->-r requirements.txt (line 1)) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->-r requirements.txt (line 1)) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 1)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->-r requirements.txt (line 1)) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->-r requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->-r requirements.txt (line 1)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->-r requirements.txt (line 1)) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow->-r requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow->-r requirements.txt (line 1)) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Đường dẫn tới dữ liệu\n",
        "image_dir = \"/content/drive/MyDrive/disaster/Comprehensive Disaster Dataset(CDD)\"\n",
        "\n",
        "# Kích thước ảnh chuẩn hóa\n",
        "IMG_SIZE = 128  # Kích thước chuẩn hóa ảnh về 128x128\n",
        "\n",
        "# Danh sách lưu đường dẫn ảnh và nhãn\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Lấy danh sách các nhãn (labels) từ thư mục\n",
        "all_labels = os.listdir(image_dir)\n",
        "\n",
        "# Duyệt qua các thư mục cha (Non_Damage và Disaster)\n",
        "for label in all_labels:\n",
        "    label_path = os.path.join(image_dir, label)\n",
        "    if os.path.isdir(label_path):  # Kiểm tra nếu là thư mục\n",
        "        if label == \"Non_Damage\":\n",
        "            # Duyệt qua tất cả các ảnh trong thư mục \"Non_Damage\"\n",
        "            for image_name in os.listdir(label_path):\n",
        "                image_paths.append(os.path.join(label_path, image_name))\n",
        "                labels.append(0)  # Gán nhãn \"Non Damage\"\n",
        "        elif label == \"Disaster\":\n",
        "            # Duyệt qua tất cả các ảnh trong thư mục \"Disaster\"\n",
        "            for image_name in os.listdir(label_path):\n",
        "                image_paths.append(os.path.join(label_path, image_name))\n",
        "                labels.append(1)  # Gán nhãn \"Disaster\"\n",
        "\n",
        "# Chuyển nhãn thành numpy array\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Chia dữ liệu thành tập huấn luyện và kiểm tra\n",
        "X_train, X_test, y_train, y_test = train_test_split(image_paths, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tiền xử lý ảnh: Chuyển ảnh thành mảng numpy, thay đổi kích thước và chuẩn hóa\n",
        "def preprocess_images(image_paths, labels):\n",
        "    images = []\n",
        "    filtered_labels = []  # List to hold corresponding labels for valid images\n",
        "    error_count = 0\n",
        "    for path, label in zip(image_paths, labels):\n",
        "        try:\n",
        "            img = Image.open(path)\n",
        "            img = img.resize((IMG_SIZE, IMG_SIZE))  # Đảm bảo kích thước ảnh\n",
        "            if img.mode != 'RGB':\n",
        "                img = img.convert('RGB')  # Chuyển đổi ảnh thành RGB nếu không phải\n",
        "            img = np.array(img)\n",
        "            if img.shape == (IMG_SIZE, IMG_SIZE, 3):\n",
        "                images.append(img)\n",
        "                filtered_labels.append(label)\n",
        "            else:\n",
        "                error_count += 1\n",
        "        except Exception as e:\n",
        "            error_count += 1\n",
        "\n",
        "    if error_count > 0:\n",
        "        print(f\"{error_count} images were skipped due to errors.\")\n",
        "\n",
        "    return np.array(images), np.array(filtered_labels)\n",
        "\n",
        "# Tiền xử lý ảnh\n",
        "X_train, y_train = preprocess_images(X_train, y_train)\n",
        "X_test, y_test = preprocess_images(X_test, y_test)\n",
        "\n",
        "# Chuẩn hóa dữ liệu\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Chuyển nhãn thành dạng one-hot encoding\n",
        "y_train = to_categorical(y_train, 2)  # Chỉ có 2 lớp (Non Damage, Disaster)\n",
        "y_test = to_categorical(y_test, 2)\n",
        "\n",
        "# Tạo đối tượng ImageDataGenerator để tăng cường dữ liệu\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Fit dữ liệu tăng cường trên tập huấn luyện\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Sử dụng mô hình ResNet50 đã được huấn luyện trước (pretrained)\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "# Đóng băng các lớp của mô hình cơ bản\n",
        "base_model.trainable = False\n",
        "\n",
        "# Xây dựng mô hình CNN mới với các lớp bổ sung\n",
        "model = Sequential([\n",
        "    base_model,  # Thêm mô hình đã huấn luyện trước\n",
        "    GlobalAveragePooling2D(),  # Global pooling thay vì Flatten\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(2, activation='softmax')  # Chỉ có 2 lớp (Non Damage, Disaster)\n",
        "])\n",
        "\n",
        "# Biên dịch mô hình\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Sử dụng EarlyStopping để tránh overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Tạo callback để lưu mô hình tốt nhất dựa trên độ chính xác xác nhận (val_accuracy)\n",
        "checkpoint = ModelCheckpoint(\"best_model.keras\", monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "\n",
        "# Huấn luyện mô hình\n",
        "model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "          validation_data=(X_test, y_test),\n",
        "          epochs=50,\n",
        "          callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "# Dự đoán trên tập kiểm tra\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# In ma trận nhầm lẫn\n",
        "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred_labels))\n",
        "\n",
        "# In báo cáo phân loại\n",
        "print(\"\\nBáo cáo phân loại:\")\n",
        "print(classification_report(np.argmax(y_test, axis=1), y_pred_labels, target_names=[\"Non Damage\", \"Disaster\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df9JTTLdf91R",
        "outputId": "19ccaa84-3736-4744-fb4a-32ad396f9b25",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 images were skipped due to errors.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m191/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.6674 - loss: 0.6457\n",
            "Epoch 1: val_accuracy improved from -inf to 0.66884, saving model to best_model.keras\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 209ms/step - accuracy: 0.6675 - loss: 0.6455 - val_accuracy: 0.6688 - val_loss: 0.6257\n",
            "Epoch 2/50\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.6871 - loss: 0.6206\n",
            "Epoch 2: val_accuracy improved from 0.66884 to 0.66949, saving model to best_model.keras\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 152ms/step - accuracy: 0.6872 - loss: 0.6206 - val_accuracy: 0.6695 - val_loss: 0.6170\n",
            "Epoch 3/50\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.6943 - loss: 0.6131\n",
            "Epoch 3: val_accuracy did not improve from 0.66949\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 149ms/step - accuracy: 0.6942 - loss: 0.6131 - val_accuracy: 0.6695 - val_loss: 0.6068\n",
            "Epoch 4/50\n",
            "\u001b[1m191/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.6863 - loss: 0.6119\n",
            "Epoch 4: val_accuracy did not improve from 0.66949\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 153ms/step - accuracy: 0.6864 - loss: 0.6118 - val_accuracy: 0.6695 - val_loss: 0.5959\n",
            "Epoch 5/50\n",
            "\u001b[1m191/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.6980 - loss: 0.5882\n",
            "Epoch 5: val_accuracy did not improve from 0.66949\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 152ms/step - accuracy: 0.6979 - loss: 0.5882 - val_accuracy: 0.6695 - val_loss: 0.5951\n",
            "Epoch 6/50\n",
            "\u001b[1m191/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.6896 - loss: 0.6018\n",
            "Epoch 6: val_accuracy improved from 0.66949 to 0.67080, saving model to best_model.keras\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 151ms/step - accuracy: 0.6896 - loss: 0.6018 - val_accuracy: 0.6708 - val_loss: 0.5828\n",
            "Epoch 7/50\n",
            "\u001b[1m191/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.6982 - loss: 0.5835\n",
            "Epoch 7: val_accuracy did not improve from 0.67080\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 154ms/step - accuracy: 0.6981 - loss: 0.5835 - val_accuracy: 0.6695 - val_loss: 0.5661\n",
            "Epoch 8/50\n",
            "\u001b[1m191/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.6977 - loss: 0.5741\n",
            "Epoch 8: val_accuracy improved from 0.67080 to 0.67405, saving model to best_model.keras\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 151ms/step - accuracy: 0.6977 - loss: 0.5742 - val_accuracy: 0.6741 - val_loss: 0.5567\n",
            "Epoch 9/50\n",
            "\u001b[1m191/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.6964 - loss: 0.5756\n",
            "Epoch 9: val_accuracy did not improve from 0.67405\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 147ms/step - accuracy: 0.6965 - loss: 0.5755 - val_accuracy: 0.6734 - val_loss: 0.5599\n",
            "Epoch 10/50\n",
            "\u001b[1m191/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.7084 - loss: 0.5623\n",
            "Epoch 10: val_accuracy did not improve from 0.67405\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 146ms/step - accuracy: 0.7084 - loss: 0.5624 - val_accuracy: 0.6708 - val_loss: 0.5599\n",
            "Epoch 11/50\n",
            "\u001b[1m191/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.7068 - loss: 0.5550\n",
            "Epoch 11: val_accuracy improved from 0.67405 to 0.68579, saving model to best_model.keras\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 154ms/step - accuracy: 0.7067 - loss: 0.5550 - val_accuracy: 0.6858 - val_loss: 0.5356\n",
            "Epoch 12/50\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.6990 - loss: 0.5582\n",
            "Epoch 12: val_accuracy improved from 0.68579 to 0.69687, saving model to best_model.keras\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 149ms/step - accuracy: 0.6991 - loss: 0.5582 - val_accuracy: 0.6969 - val_loss: 0.5319\n",
            "Epoch 13/50\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.7066 - loss: 0.5521\n",
            "Epoch 13: val_accuracy improved from 0.69687 to 0.70143, saving model to best_model.keras\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 157ms/step - accuracy: 0.7066 - loss: 0.5521 - val_accuracy: 0.7014 - val_loss: 0.5282\n",
            "Epoch 14/50\n",
            "\u001b[1m191/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.7220 - loss: 0.5425\n",
            "Epoch 14: val_accuracy improved from 0.70143 to 0.70665, saving model to best_model.keras\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 151ms/step - accuracy: 0.7218 - loss: 0.5425 - val_accuracy: 0.7066 - val_loss: 0.5225\n",
            "Epoch 15/50\n",
            "\u001b[1m191/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.7170 - loss: 0.5434\n",
            "Epoch 15: val_accuracy did not improve from 0.70665\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 147ms/step - accuracy: 0.7169 - loss: 0.5434 - val_accuracy: 0.6773 - val_loss: 0.5467\n",
            "Epoch 16/50\n",
            "\u001b[1m191/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.7158 - loss: 0.5371\n",
            "Epoch 16: val_accuracy did not improve from 0.70665\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 155ms/step - accuracy: 0.7157 - loss: 0.5372 - val_accuracy: 0.6917 - val_loss: 0.5394\n",
            "Epoch 17/50\n",
            "\u001b[1m191/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.7136 - loss: 0.5333\n",
            "Epoch 17: val_accuracy improved from 0.70665 to 0.70730, saving model to best_model.keras\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 151ms/step - accuracy: 0.7137 - loss: 0.5333 - val_accuracy: 0.7073 - val_loss: 0.5239\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 100ms/step\n",
            "[[1011   15]\n",
            " [ 435   73]]\n",
            "\n",
            "Báo cáo phân loại:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Non Damage       0.70      0.99      0.82      1026\n",
            "    Disaster       0.83      0.14      0.24       508\n",
            "\n",
            "    accuracy                           0.71      1534\n",
            "   macro avg       0.76      0.56      0.53      1534\n",
            "weighted avg       0.74      0.71      0.63      1534\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Đường dẫn tới mô hình đã lưu\n",
        "model_path = \"best_model.keras\"  # Đường dẫn tới mô hình bạn đã lưu\n",
        "\n",
        "# Tải mô hình đã huấn luyện\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Kích thước ảnh chuẩn hóa\n",
        "IMG_SIZE = 128  # Chỉnh kích thước ảnh về 128x128\n",
        "\n",
        "# Tiền xử lý ảnh mới (cũng giống như tiền xử lý khi huấn luyện)\n",
        "def preprocess_image(img_path):\n",
        "    img = Image.open(img_path)\n",
        "    img = img.resize((IMG_SIZE, IMG_SIZE))  # Đảm bảo kích thước ảnh\n",
        "    if img.mode != 'RGB':\n",
        "        img = img.convert('RGB')  # Chuyển đổi ảnh thành RGB nếu không phải\n",
        "    img = np.array(img)\n",
        "\n",
        "    # Chuẩn hóa dữ liệu\n",
        "    img = img / 255.0\n",
        "\n",
        "    # Thêm chiều mới để giống như dữ liệu đầu vào (batch size = 1)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "\n",
        "    return img\n",
        "\n",
        "# Dự đoán cho một ảnh mới\n",
        "def predict_single_image(img_path):\n",
        "    # Tiền xử lý ảnh\n",
        "    img = preprocess_image(img_path)\n",
        "\n",
        "    # Dự đoán nhãn của ảnh\n",
        "    prediction = model.predict(img)\n",
        "    predicted_label = np.argmax(prediction, axis=1)\n",
        "\n",
        "    # In kết quả dự đoán\n",
        "    print(f\"Dự đoán nhãn: {'Disaster' if predicted_label == 0 else 'Non Damage'}\")\n",
        "\n",
        "# Dự đoán cho một ảnh mới (thay đổi đường dẫn ảnh)\n",
        "img_path = \"/content/drive/MyDrive/disaster/Comprehensive Disaster Dataset(CDD)/Non_Damage/06_01_0001.png\"  # Thay bằng đường dẫn ảnh bạn muốn kiểm thử\n",
        "predict_single_image(img_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI7qoji-n3do",
        "outputId": "66862f7a-6f66-4ca6-be66-6318b7126332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Dự đoán nhãn: Non Damage\n"
          ]
        }
      ]
    }
  ]
}